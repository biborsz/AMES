{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "\n",
    "data = pd.read_csv('./data/reddit_working_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47641</th>\n",
       "      <td>1421701672</td>\n",
       "      <td>how do they get the particles and find their e...</td>\n",
       "      <td>askscience</td>\n",
       "      <td>How do people who study entangled particles ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47642</th>\n",
       "      <td>1417723027</td>\n",
       "      <td>the delta iv rocket isn t that new  so what is...</td>\n",
       "      <td>askscience</td>\n",
       "      <td>What is the goal/purpose of the Orion test lau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47643</th>\n",
       "      <td>1519983877</td>\n",
       "      <td>this doesn t seem to be true of all icicles  e...</td>\n",
       "      <td>askscience</td>\n",
       "      <td>Why do large icicles appear ribbed?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47644</th>\n",
       "      <td>1432885551</td>\n",
       "      <td>i know that nature only deals with the d and l...</td>\n",
       "      <td>askscience</td>\n",
       "      <td>What would happen if you tried to eat a sandwi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47645</th>\n",
       "      <td>1491402075</td>\n",
       "      <td>i ve seen the maps of the ice sheets in north ...</td>\n",
       "      <td>askscience</td>\n",
       "      <td>Why was the southern hemisphere ice sheet smal...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       created_utc                                           selftext  \\\n",
       "47641   1421701672  how do they get the particles and find their e...   \n",
       "47642   1417723027  the delta iv rocket isn t that new  so what is...   \n",
       "47643   1519983877  this doesn t seem to be true of all icicles  e...   \n",
       "47644   1432885551  i know that nature only deals with the d and l...   \n",
       "47645   1491402075  i ve seen the maps of the ice sheets in north ...   \n",
       "\n",
       "        subreddit                                              title  \n",
       "47641  askscience  How do people who study entangled particles ge...  \n",
       "47642  askscience  What is the goal/purpose of the Orion test lau...  \n",
       "47643  askscience                Why do large icicles appear ribbed?  \n",
       "47644  askscience  What would happen if you tried to eat a sandwi...  \n",
       "47645  askscience  Why was the southern hemisphere ice sheet smal...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect scifi data - first and last five rows\n",
    "\n",
    "data.head()\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function - to inspect dataframe\n",
    "\n",
    "def inspect(dataframe):\n",
    "    print('Rows, columns:', dataframe.shape)\n",
    "    print('')\n",
    "    print(dataframe.info())      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows, columns: (47646, 4)\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 47646 entries, 0 to 47645\n",
      "Data columns (total 4 columns):\n",
      "created_utc    47646 non-null int64\n",
      "selftext       47646 non-null object\n",
      "subreddit      47646 non-null object\n",
      "title          47646 non-null object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 1.5+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# print info about dataframe\n",
    "\n",
    "inspect(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recode subreddit variable \n",
    "# scifi = 0, askscience = 1\n",
    "\n",
    "data['is_science'] = data['subreddit'].map({'askscience': 1, 'scifi': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define X and y variables\n",
    "\n",
    "X = data['selftext']\n",
    "\n",
    "y = data['is_science']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test-split the data\n",
    "# stratify split - for balanced test set\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47374    1\n",
       "21288    0\n",
       "27823    1\n",
       "25463    1\n",
       "18991    0\n",
       "Name: is_science, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train.head()\n",
    "X_test.head()\n",
    "y_train.head()\n",
    "y_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function that instantiates pipeline\n",
    "# arguments: transformer, estimator\n",
    "\n",
    "def piper(transformer, estimator):\n",
    "    pipe = make_pipeline(transformer, estimator)\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters for gridsearch\n",
    "\n",
    "def pipe_params(transformer):\n",
    "    transf = str(transformer).split('(')[0].lower()\n",
    "    params = {\n",
    "        transf + '__max_features': [100, 500],\n",
    "        transf + '__stop_words': ['english', None],\n",
    "        transf + '__ngram_range': [(1, 1), (1, 2), (1, 3)]\n",
    "    }\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function: arguments = pipeline, params\n",
    "# runs gridsearch cv\n",
    "# gives back: gridsearch cv score, best parameters\n",
    "\n",
    "def gridsearch(transformer, estimator):\n",
    "\n",
    "    pipe = piper(transformer, estimator)\n",
    "    params = pipe_params(transformer)\n",
    "    gs = GridSearchCV(pipe,\n",
    "                      param_grid=params,\n",
    "                      cv=5,\n",
    "                      n_jobs=-1)\n",
    "    gs.fit(X_train, y_train)\n",
    "    best_model = gs.best_estimator_\n",
    "    \n",
    "    print(str(transformer).split('(')[0])\n",
    "    print('Best model:', gs.best_params_)\n",
    "    print('')\n",
    "    print('Best model:', best_model)\n",
    "    print('')\n",
    "    print('Cross-val score of the best model:', gs.best_score_)\n",
    "    print('Accuracy of best model on the training data:', best_model.score(X_train, y_train))\n",
    "    print('Accuracy of best model on the testing data:', best_model.score(X_test, y_test))\n",
    "    print('')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Baseline Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.504113\n",
       "0    0.495887\n",
       "Name: is_science, dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# value count on y_test\n",
    "\n",
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we predict for every single submission that it came from the askscience subreddit - we will be right 50.41% of the time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model 1** <br>\n",
    "*count vectorizer - logistic regression*\n",
    "\n",
    "    parameters: \n",
    "        - stop words: english, None \n",
    "        - max_features: 100, 500 \n",
    "        - ngrams: 1-grams, 2-grams, 3-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer\n",
      "\n",
      "Best model: Pipeline(memory=None,\n",
      "         steps=[('countvectorizer',\n",
      "                 CountVectorizer(analyzer='word', binary=False,\n",
      "                                 decode_error='strict',\n",
      "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
      "                                 input='content', lowercase=True, max_df=1.0,\n",
      "                                 max_features=500, min_df=1, ngram_range=(1, 1),\n",
      "                                 preprocessor=None, stop_words=None,\n",
      "                                 strip_accents=None,\n",
      "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                                 tokenizer=None, vocabulary=None)),\n",
      "                ('logisticregression',\n",
      "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
      "                                    fit_intercept=True, intercept_scaling=1,\n",
      "                                    l1_ratio=None, max_iter=500,\n",
      "                                    multi_class='auto', n_jobs=None,\n",
      "                                    penalty='l2', random_state=None,\n",
      "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                                    warm_start=False))],\n",
      "         verbose=False)\n",
      "\n",
      "Cross-val score of the best model: 0.9088822778897951\n",
      "Accuracy of best model on the training data: 0.9150668830805395\n",
      "Accuracy of best model on the testing data: 0.9101746138347885\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run the gridsearch function with CountVectorizer and Logistic Regression\n",
    "\n",
    "gridsearch(CountVectorizer(), LogisticRegression(max_iter=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best parameters: stop_words = None, max_features = 500, ngrams = 1-grams <br>\n",
    "Cross-Val accuracy: 90.9%, Training accuracy: 91.5%, Testing accuracy: 91%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model 2** <br>\n",
    "*tfidf-vectorizer - logistic regression*\n",
    "\n",
    "    parameters: \n",
    "        - stop words: english, None \n",
    "        - max_features: 100, 500\n",
    "        - ngrams: 1-grams, 2-grams, 3-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer\n",
      "\n",
      "Best model: Pipeline(memory=None,\n",
      "         steps=[('tfidfvectorizer',\n",
      "                 TfidfVectorizer(analyzer='word', binary=False,\n",
      "                                 decode_error='strict',\n",
      "                                 dtype=<class 'numpy.float64'>,\n",
      "                                 encoding='utf-8', input='content',\n",
      "                                 lowercase=True, max_df=1.0, max_features=500,\n",
      "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
      "                                 preprocessor=None, smooth_idf=True,\n",
      "                                 stop_words=None, strip_accents=None,\n",
      "                                 sublinear_tf=False,\n",
      "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                                 tokenizer=None, use_idf=True,\n",
      "                                 vocabulary=None)),\n",
      "                ('logisticregression',\n",
      "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
      "                                    fit_intercept=True, intercept_scaling=1,\n",
      "                                    l1_ratio=None, max_iter=500,\n",
      "                                    multi_class='auto', n_jobs=None,\n",
      "                                    penalty='l2', random_state=None,\n",
      "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                                    warm_start=False))],\n",
      "         verbose=False)\n",
      "\n",
      "Cross-val score of the best model: 0.9128840548160768\n",
      "Accuracy of best model on the training data: 0.9184250293837801\n",
      "Accuracy of best model on the testing data: 0.9133646742780389\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run gridsearch function on TfidfVectorizer and LogisticRegression\n",
    "\n",
    "gridsearch(TfidfVectorizer(), LogisticRegression(max_iter=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best parameters: stop_words = None, max_features = 500, ngrams = 1-grams <br>\n",
    "Cross-Val accuracy: 91.28%, Training accuracy: 91.8%, Testing accuracy: 91.34%\n",
    "Comparing the cross-val accuracy score with the 90.9% of Model 1 - Model 2 performed slightly (0.38 percentage points) better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model 3** <br>\n",
    "*count vectorizer - Bernoulli Naive Bayes*\n",
    "\n",
    "    parameters: \n",
    "        - stop words: english, None \n",
    "        - max_features: 100, 500 \n",
    "        - ngrams: 1-grams, 2-grams, 3-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bibor/opt/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer\n",
      "\n",
      "Best model: Pipeline(memory=None,\n",
      "         steps=[('countvectorizer',\n",
      "                 CountVectorizer(analyzer='word', binary=False,\n",
      "                                 decode_error='strict',\n",
      "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
      "                                 input='content', lowercase=True, max_df=1.0,\n",
      "                                 max_features=500, min_df=1, ngram_range=(1, 2),\n",
      "                                 preprocessor=None, stop_words='english',\n",
      "                                 strip_accents=None,\n",
      "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                                 tokenizer=None, vocabulary=None)),\n",
      "                ('bernoullinb',\n",
      "                 BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None,\n",
      "                             fit_prior=True))],\n",
      "         verbose=False)\n",
      "\n",
      "Cross-val score of the best model: 0.8451054425377027\n",
      "Accuracy of best model on the training data: 0.8467845749146471\n",
      "Accuracy of best model on the testing data: 0.8469610476830087\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run gridsearch function on CountVectorizer and BernoulliNB (a binary classifier)\n",
    "\n",
    "gridsearch(CountVectorizer(), BernoulliNB())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best parameters: stop_words = English, max_features = 500, ngrams = 2-grams <br>\n",
    "Cross-Val accuracy: 84.51%, Training accuracy: 84.68%, Testing accuracy: 84.7%\n",
    "Comparing the cross-val accuracy scores with the 91.28% of Model 2 (most accurate model up to this point) - Model 3 performed 6.77 percentage points worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model 4** <br>\n",
    "*tfidf-vectorizer - Bernoulli Naive Bayes*\n",
    "\n",
    "    parameters: \n",
    "        - stop words: english, None \n",
    "        - max_features: 100, 500 \n",
    "        - ngrams: 1-grams, 2-grams, 3-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer\n",
      "\n",
      "Best model: Pipeline(memory=None,\n",
      "         steps=[('tfidfvectorizer',\n",
      "                 TfidfVectorizer(analyzer='word', binary=False,\n",
      "                                 decode_error='strict',\n",
      "                                 dtype=<class 'numpy.float64'>,\n",
      "                                 encoding='utf-8', input='content',\n",
      "                                 lowercase=True, max_df=1.0, max_features=500,\n",
      "                                 min_df=1, ngram_range=(1, 2), norm='l2',\n",
      "                                 preprocessor=None, smooth_idf=True,\n",
      "                                 stop_words='english', strip_accents=None,\n",
      "                                 sublinear_tf=False,\n",
      "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                                 tokenizer=None, use_idf=True,\n",
      "                                 vocabulary=None)),\n",
      "                ('bernoullinb',\n",
      "                 BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None,\n",
      "                             fit_prior=True))],\n",
      "         verbose=False)\n",
      "\n",
      "Cross-val score of the best model: 0.8451054425377027\n",
      "Accuracy of best model on the training data: 0.8467845749146471\n",
      "Accuracy of best model on the testing data: 0.8469610476830087\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run gridsearch function on TfidfVectorizer and BernoulliNB (a binary classifier)\n",
    "\n",
    "gridsearch(TfidfVectorizer(), BernoulliNB())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best parameters: stop_words = English, max_features = 500, ngrams = 2-grams <br>\n",
    "Cross-Val accuracy: 84.51%, Training accuracy: 84.68%, Testing accuracy: 84.7%\n",
    "Comparing the cross-val accuracy scores with the 91.28% of Model 2 (most accurate model up to this point) - Model 3 performed 6.77 percentage points worse. (The Bernoulli NB models with CountVectorizer and with Tfidf Vectorizer performed exactly the same; this was because the Bernoulli NB model classifies based on whether a feature is in a certain document or it is missing from there.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model 5** <br>\n",
    "*count vectorizer - Multinomial Naive Bayes*\n",
    "\n",
    "    parameters: \n",
    "        - stop words: english, None \n",
    "        - max_features: 100, 500 \n",
    "        - ngrams: 1-grams, 2-grams, 3-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bibor/opt/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer\n",
      "\n",
      "Best model: Pipeline(memory=None,\n",
      "         steps=[('countvectorizer',\n",
      "                 CountVectorizer(analyzer='word', binary=False,\n",
      "                                 decode_error='strict',\n",
      "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
      "                                 input='content', lowercase=True, max_df=1.0,\n",
      "                                 max_features=500, min_df=1, ngram_range=(1, 1),\n",
      "                                 preprocessor=None, stop_words=None,\n",
      "                                 strip_accents=None,\n",
      "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                                 tokenizer=None, vocabulary=None)),\n",
      "                ('multinomialnb',\n",
      "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
      "         verbose=False)\n",
      "\n",
      "Cross-val score of the best model: 0.9023338682987321\n",
      "Accuracy of best model on the training data: 0.9028936027312924\n",
      "Accuracy of best model on the testing data: 0.9021155137676293\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run gridsearch\n",
    "\n",
    "gridsearch(CountVectorizer(), MultinomialNB())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best parameters: stop_words = None, max_features = 500, ngrams = 1-grams <br>\n",
    "Cross-Val accuracy: 90.23%, Training accuracy: 90.29%, Testing accuracy: 90.21%\n",
    "Comparing the cross-val accuracy scores with the 91.28% of Model 2 (most accurate model up to this point) - Model 3 performed 1.05 percentage points worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model 6** <br>\n",
    "*Tfidf vectorizer - Multinomial Naive Bayes*\n",
    "\n",
    "    parameters: \n",
    "        - stop words: english, None \n",
    "        - max_features: 100, 500 \n",
    "        - ngrams: 1-grams, 2-grams, 3-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bibor/opt/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer\n",
      "\n",
      "Best model: Pipeline(memory=None,\n",
      "         steps=[('tfidfvectorizer',\n",
      "                 TfidfVectorizer(analyzer='word', binary=False,\n",
      "                                 decode_error='strict',\n",
      "                                 dtype=<class 'numpy.float64'>,\n",
      "                                 encoding='utf-8', input='content',\n",
      "                                 lowercase=True, max_df=1.0, max_features=500,\n",
      "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
      "                                 preprocessor=None, smooth_idf=True,\n",
      "                                 stop_words=None, strip_accents=None,\n",
      "                                 sublinear_tf=False,\n",
      "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                                 tokenizer=None, use_idf=True,\n",
      "                                 vocabulary=None)),\n",
      "                ('multinomialnb',\n",
      "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
      "         verbose=False)\n",
      "\n",
      "Cross-val score of the best model: 0.9044046985633862\n",
      "Accuracy of best model on the training data: 0.9050484132758717\n",
      "Accuracy of best model on the testing data: 0.9047179314976495\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run gridsearch\n",
    "\n",
    "gridsearch(TfidfVectorizer(), MultinomialNB())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best parameters: stop_words = None, max_features = 500, ngrams = 1-grams <br>\n",
    "Cross-Val accuracy: 90.44%, Training accuracy: 90.50%, Testing accuracy: 90.47%\n",
    "Comparing the cross-val accuracy scores with the 91.28% of Model 2 (most accurate model up to this point) - Model 6 performed 0.84 percentage points worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Decision Tree Models***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model 7** <br>\n",
    "*count vectorizer - decision tree*\n",
    "\n",
    "    parameters: \n",
    "        - stop words: english, None \n",
    "        - max_features: 100, 500 \n",
    "        - ngrams: 1-grams, 2-grams, 3-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bibor/opt/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer\n",
      "\n",
      "Best model: Pipeline(memory=None,\n",
      "         steps=[('countvectorizer',\n",
      "                 CountVectorizer(analyzer='word', binary=False,\n",
      "                                 decode_error='strict',\n",
      "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
      "                                 input='content', lowercase=True, max_df=1.0,\n",
      "                                 max_features=100, min_df=1, ngram_range=(1, 1),\n",
      "                                 preprocessor=None, stop_words='english',\n",
      "                                 strip_accents=None,\n",
      "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                                 tokenizer=None...lary=None)),\n",
      "                ('decisiontreeclassifier',\n",
      "                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,\n",
      "                                        criterion='gini', max_depth=2,\n",
      "                                        max_features=None, max_leaf_nodes=None,\n",
      "                                        min_impurity_decrease=0.0,\n",
      "                                        min_impurity_split=None,\n",
      "                                        min_samples_leaf=1, min_samples_split=2,\n",
      "                                        min_weight_fraction_leaf=0.0,\n",
      "                                        presort='deprecated', random_state=None,\n",
      "                                        splitter='best'))],\n",
      "         verbose=False)\n",
      "\n",
      "Cross-val score of the best model: 0.6516202410606327\n",
      "Accuracy of best model on the training data: 0.6516203055913136\n",
      "Accuracy of best model on the testing data: 0.6558092679650772\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run gridsearch\n",
    "\n",
    "gridsearch(CountVectorizer(), DecisionTreeClassifier(max_depth=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best parameters: stop_words = English, max_features = 100, ngrams = 1-grams <br>\n",
    "Cross-Val accuracy: 65.16%, Training accuracy: 65.16%, Testing accuracy: 65.58%\n",
    "Comparing the cross-val accuracy scores with the 91.28% of Model 2 (most accurate model up to this point) - Model 7 performed 26.12 percentage points worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model 8** <br>\n",
    "*tfidf vectorizer - decision tree*\n",
    "\n",
    "    parameters: \n",
    "        - stop words: english, None \n",
    "        - max_features: 100, 500 \n",
    "        - ngrams: 1-grams, 2-grams, 3-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bibor/opt/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer\n",
      "\n",
      "Best model: Pipeline(memory=None,\n",
      "         steps=[('tfidfvectorizer',\n",
      "                 TfidfVectorizer(analyzer='word', binary=False,\n",
      "                                 decode_error='strict',\n",
      "                                 dtype=<class 'numpy.float64'>,\n",
      "                                 encoding='utf-8', input='content',\n",
      "                                 lowercase=True, max_df=1.0, max_features=100,\n",
      "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
      "                                 preprocessor=None, smooth_idf=True,\n",
      "                                 stop_words='english', strip_accents=None,\n",
      "                                 sublinear_tf=False,\n",
      "                                 t...\n",
      "                                 vocabulary=None)),\n",
      "                ('decisiontreeclassifier',\n",
      "                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,\n",
      "                                        criterion='gini', max_depth=4,\n",
      "                                        max_features=None, max_leaf_nodes=None,\n",
      "                                        min_impurity_decrease=0.0,\n",
      "                                        min_impurity_split=None,\n",
      "                                        min_samples_leaf=1, min_samples_split=2,\n",
      "                                        min_weight_fraction_leaf=0.0,\n",
      "                                        presort='deprecated', random_state=None,\n",
      "                                        splitter='best'))],\n",
      "         verbose=False)\n",
      "\n",
      "Cross-val score of the best model: 0.7262831112390862\n",
      "Accuracy of best model on the training data: 0.7268147982313763\n",
      "Accuracy of best model on the testing data: 0.7305238415043653\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run gridsearch\n",
    "\n",
    "gridsearch(TfidfVectorizer(), DecisionTreeClassifier(max_depth=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best parameters: stop_words = English, max_features = 100, ngrams = 1-grams <br>\n",
    "Cross-Val accuracy: 72.62%, Training accuracy: 72.68%, Testing accuracy: 73.05%\n",
    "Comparing the cross-val accuracy scores with the 91.28% of Model 2 (most accurate model up to this point) - Model 8 performed 18.66 percentage points worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model 9** <br>\n",
    "*count vectorizer - ada boost*\n",
    "\n",
    "    parameters: \n",
    "        - stop words: english, None \n",
    "        - max_features: 100, 500 \n",
    "        - ngrams: 1-grams, 2-grams, 3-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bibor/opt/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer\n",
      "Best model: {'countvectorizer__max_features': 500, 'countvectorizer__ngram_range': (1, 2), 'countvectorizer__stop_words': None}\n",
      "\n",
      "Best model: Pipeline(memory=None,\n",
      "         steps=[('countvectorizer',\n",
      "                 CountVectorizer(analyzer='word', binary=False,\n",
      "                                 decode_error='strict',\n",
      "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
      "                                 input='content', lowercase=True, max_df=1.0,\n",
      "                                 max_features=500, min_df=1, ngram_range=(1, 2),\n",
      "                                 preprocessor=None, stop_words=None,\n",
      "                                 strip_accents=None,\n",
      "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                                 tokenizer=None, vocabulary=None)),\n",
      "                ('adaboostclassifier',\n",
      "                 AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "                                    learning_rate=1.0, n_estimators=50,\n",
      "                                    random_state=None))],\n",
      "         verbose=False)\n",
      "\n",
      "Cross-val score of the best model: 0.875216992672098\n",
      "Accuracy of best model on the training data: 0.8769799070912856\n",
      "Accuracy of best model on the testing data: 0.8756715916722633\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run gridsearch\n",
    "\n",
    "gridsearch(CountVectorizer(), AdaBoostClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best parameters: stop_words = None, max_features = 500, ngrams = 2-grams <br>\n",
    "Cross-Val accuracy: 87.52%, Training accuracy: 87.7%, Testing accuracy: 87.57%\n",
    "Comparing the cross-val accuracy scores with the 91.28% of Model 2 (most accurate model up to this point) - Model 9 performed 3.76 percentage points worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model 10** <br>\n",
    "*tfidf vectorizer - ada boost*\n",
    "\n",
    "    parameters: \n",
    "        - stop words: english, None \n",
    "        - max_features: 100, 500 \n",
    "        - ngrams: 1-grams, 2-grams, 3-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bibor/opt/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer\n",
      "Best model: {'tfidfvectorizer__max_features': 500, 'tfidfvectorizer__ngram_range': (1, 2), 'tfidfvectorizer__stop_words': None}\n",
      "\n",
      "Best model: Pipeline(memory=None,\n",
      "         steps=[('tfidfvectorizer',\n",
      "                 TfidfVectorizer(analyzer='word', binary=False,\n",
      "                                 decode_error='strict',\n",
      "                                 dtype=<class 'numpy.float64'>,\n",
      "                                 encoding='utf-8', input='content',\n",
      "                                 lowercase=True, max_df=1.0, max_features=500,\n",
      "                                 min_df=1, ngram_range=(1, 2), norm='l2',\n",
      "                                 preprocessor=None, smooth_idf=True,\n",
      "                                 stop_words=None, strip_accents=None,\n",
      "                                 sublinear_tf=False,\n",
      "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                                 tokenizer=None, use_idf=True,\n",
      "                                 vocabulary=None)),\n",
      "                ('adaboostclassifier',\n",
      "                 AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "                                    learning_rate=1.0, n_estimators=50,\n",
      "                                    random_state=None))],\n",
      "         verbose=False)\n",
      "\n",
      "Cross-val score of the best model: 0.8788829565334055\n",
      "Accuracy of best model on the training data: 0.8800302233167292\n",
      "Accuracy of best model on the testing data: 0.8798690396239086\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run gridsearch\n",
    "\n",
    "gridsearch(TfidfVectorizer(), AdaBoostClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best parameters: stop_words = None, max_features = 500, ngrams = 2-grams <br>\n",
    "Cross-Val accuracy: 87.89%, Training accuracy: 88%, Testing accuracy: 87.99%\n",
    "Comparing the cross-val accuracy scores with the 91.28% of Model 2 (most accurate model up to this point) - Model 10 performed 3.39 percentage points worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model 11** <br>\n",
    "*tfidf-vectorizer - AdaBoost - decision tree*\n",
    "\n",
    "    parameters: \n",
    "        - stop words: english, None \n",
    "        - max_features: 100, 500 \n",
    "        - ngrams: 1-grams, 2-grams, 3-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bibor/opt/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer\n",
      "Best model: {'tfidfvectorizer__max_features': 500, 'tfidfvectorizer__ngram_range': (1, 1), 'tfidfvectorizer__stop_words': None}\n",
      "\n",
      "Best model: Pipeline(memory=None,\n",
      "         steps=[('tfidfvectorizer',\n",
      "                 TfidfVectorizer(analyzer='word', binary=False,\n",
      "                                 decode_error='strict',\n",
      "                                 dtype=<class 'numpy.float64'>,\n",
      "                                 encoding='utf-8', input='content',\n",
      "                                 lowercase=True, max_df=1.0, max_features=500,\n",
      "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
      "                                 preprocessor=None, smooth_idf=True,\n",
      "                                 stop_words=None, strip_accents=None,\n",
      "                                 sublinear_tf=False,\n",
      "                                 token_...\n",
      "                                    base_estimator=DecisionTreeClassifier(ccp_alpha=0.0,\n",
      "                                                                          class_weight=None,\n",
      "                                                                          criterion='gini',\n",
      "                                                                          max_depth=4,\n",
      "                                                                          max_features=None,\n",
      "                                                                          max_leaf_nodes=None,\n",
      "                                                                          min_impurity_decrease=0.0,\n",
      "                                                                          min_impurity_split=None,\n",
      "                                                                          min_samples_leaf=1,\n",
      "                                                                          min_samples_split=2,\n",
      "                                                                          min_weight_fraction_leaf=0.0,\n",
      "                                                                          presort='deprecated',\n",
      "                                                                          random_state=None,\n",
      "                                                                          splitter='best'),\n",
      "                                    learning_rate=1.0, n_estimators=50,\n",
      "                                    random_state=None))],\n",
      "         verbose=False)\n",
      "\n",
      "Cross-val score of the best model: 0.8940226065467531\n",
      "Accuracy of best model on the training data: 0.9383220462304808\n",
      "Accuracy of best model on the testing data: 0.8938885157824042\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run gridsearch\n",
    "\n",
    "gridsearch(TfidfVectorizer(), AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best parameters: stop_words = None, max_features = 500, ngrams = 1-grams <br>\n",
    "Cross-Val accuracy: 89.4%, Training accuracy: 93.8%, Testing accuracy: 89.38%\n",
    "Comparing the cross-val accuracy scores with the 91.28% of Model 2 (most accurate model up to this point) - Model 11 performed 1.88 percentage points worse. (note for myself: this model looks promising)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model 12** <br>\n",
    "*count vectorizer - gradient boosting*\n",
    "\n",
    "    parameters: \n",
    "        - stop words: english, None \n",
    "        - max_features: 100, 500 \n",
    "        - ngrams: 1-grams, 2-grams\n",
    "        - gradientboosting max_depth: 3, 4, 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate pipeline - with GradientBoostingClassifier\n",
    "\n",
    "pipe_grad = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('grad', GradientBoostingClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for gs_grad\n",
    "\n",
    "param_pipe_grad = {\n",
    "    'cvec__max_features': [100, 500],\n",
    "    'cvec__stop_words': ['english', None],\n",
    "    'cvec__ngram_range': [(1, 1), (1, 2)],\n",
    "    'grad__max_depth': [3, 4, 5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bibor/opt/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('cvec',\n",
       "                                        CountVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.int64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        preprocessor=None,\n",
       "                                                        stop_words=None,\n",
       "                                                        strip_accents=None,\n",
       "                                                        token_pattern='(?u)...\n",
       "                                                                   random_state=None,\n",
       "                                                                   subsample=1.0,\n",
       "                                                                   tol=0.0001,\n",
       "                                                                   validation_fraction=0.1,\n",
       "                                                                   verbose=0,\n",
       "                                                                   warm_start=False))],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'cvec__max_features': [100, 500],\n",
       "                         'cvec__ngram_range': [(1, 1), (1, 2)],\n",
       "                         'cvec__stop_words': ['english', None],\n",
       "                         'grad__max_depth': [3, 4, 5]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate grid search\n",
    "\n",
    "gs_grad = GridSearchCV(\n",
    "    pipe_grad,\n",
    "    param_grid=param_pipe_grad,\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "gs_grad.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: {'cvec__max_features': 500, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': None, 'grad__max_depth': 5}\n",
      "\n",
      "Cross-val score of the best model: 0.894302475568928\n",
      "Accuracy of best model on the training data: 0.9057480270890469\n",
      "Accuracy of best model on the testing data: 0.8938045668233714\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the accuracy scores of the best GradientBoost model\n",
    "\n",
    "best_model = gs_grad.best_estimator_\n",
    "print('Best model:', gs_grad.best_params_)\n",
    "print('')\n",
    "print('Cross-val score of the best model:', gs_grad.best_score_)\n",
    "print('Accuracy of best model on the training data:', best_model.score(X_train, y_train))\n",
    "print('Accuracy of best model on the testing data:', best_model.score(X_test, y_test))\n",
    "print('')\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best parameters: stop_words = None, max_features = 500, ngrams = 1-grams, gradient boosting max_depth = 5 <br>\n",
    "Cross-Val accuracy: 89.43%, Training accuracy: 90.57%, Testing accuracy: 89.38%\n",
    "Comparing the cross-val accuracy scores with the 91.28% of Model 2 (most accurate model up to this point) - Model 12 performed 1.85 percentage points worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models with Lemmatizer and Stemmer <br>\n",
    "Used best model - Model 2 - Tfidf Vectorizer - Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenize Documents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate tokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer('\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to\n",
    "# tokenize the documents\n",
    "\n",
    "def tokens(column):\n",
    "    list_of_tokens = []\n",
    "    for string in column:\n",
    "        list_of_tokens.append(tokenizer.tokenize(string))\n",
    "    return list_of_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize X_train and X_test data\n",
    "\n",
    "X_train_tokens = tokens(X_train)\n",
    "X_test_tokens = tokens(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model 13** <br>\n",
    "*lemmatizer - Tfidf-Vectorizer - logistic regression*\n",
    "\n",
    "    parameters: \n",
    "        - stop words: english, None \n",
    "        - max_features: 500, 600 \n",
    "        - ngrams: 1-grams, 2-grams\n",
    "    tokenizer: removed punctuation, all non-word character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define lemmatizer function\n",
    "\n",
    "def lemma(token_list):\n",
    "    list_of_lemmas = []\n",
    "    for token in token_list:\n",
    "        list_of_lemmas.append(lemmatizer.lemmatize(str(token)))\n",
    "    return list_of_lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize X_train and X_test tokens\n",
    "\n",
    "X_train = lemma(X_train_tokens)\n",
    "X_test = lemma(X_test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bibor/opt/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer\n",
      "Best model: {'tfidfvectorizer__max_features': 500, 'tfidfvectorizer__ngram_range': (1, 1), 'tfidfvectorizer__stop_words': None}\n",
      "\n",
      "Best model: Pipeline(memory=None,\n",
      "         steps=[('tfidfvectorizer',\n",
      "                 TfidfVectorizer(analyzer='word', binary=False,\n",
      "                                 decode_error='strict',\n",
      "                                 dtype=<class 'numpy.float64'>,\n",
      "                                 encoding='utf-8', input='content',\n",
      "                                 lowercase=True, max_df=1.0, max_features=500,\n",
      "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
      "                                 preprocessor=None, smooth_idf=True,\n",
      "                                 stop_words=None, strip_accents=None,\n",
      "                                 sublinear_tf=False,\n",
      "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                                 tokenizer=None, use_idf=True,\n",
      "                                 vocabulary=None)),\n",
      "                ('logisticregression',\n",
      "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
      "                                    fit_intercept=True, intercept_scaling=1,\n",
      "                                    l1_ratio=None, max_iter=100,\n",
      "                                    multi_class='auto', n_jobs=None,\n",
      "                                    penalty='l2', random_state=None,\n",
      "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                                    warm_start=False))],\n",
      "         verbose=False)\n",
      "\n",
      "Cross-val score of the best model: 0.9128840548160768\n",
      "Accuracy of best model on the training data: 0.9184250293837801\n",
      "Accuracy of best model on the testing data: 0.9133646742780389\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run gridsearch on TfidfVectorizer and LogisticRegressin after lemmatizing\n",
    "\n",
    "gridsearch(TfidfVectorizer(), LogisticRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best parameters: stop_words = None, max_features = 500, ngrams = 1-grams<br>\n",
    "Cross-Val accuracy: 91.28%, Training accuracy: 91.84%, Testing accuracy: 91.33%\n",
    "Comparing the cross-val accuracy scores with the 91.28% of Model 2 (most accurate model up to this point) - Model 13 performed equally well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model 14** <br>\n",
    "*stemmer - tfidf-vectorizer - logistic regression*\n",
    "\n",
    "    parameters: \n",
    "        - stop words: english, None \n",
    "        - max_features: 500, 600 \n",
    "        - ngrams: 1-grams, 2-grams, 3-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# create function to\n",
    "# stem tokenized documents\n",
    "def stem(token_list):\n",
    "    list_of_stems = []\n",
    "    for string in token_list:\n",
    "        list_of_stems.append(stemmer.stem(str(string)))\n",
    "    return list_of_stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run stemmer function on X_train_tokens\n",
    "# set X_train to stemmed tokens (to run gridsearch function)\n",
    "X_train = stem(X_train_tokens)\n",
    "\n",
    "# run stemmer function on X_test_tokens\n",
    "# set X_test to stemmed tokens (to run gridsearch function)\n",
    "X_test = stem(X_test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer\n",
      "Best model: {'tfidfvectorizer__max_features': 500, 'tfidfvectorizer__ngram_range': (1, 1), 'tfidfvectorizer__stop_words': None}\n",
      "\n",
      "Best model: Pipeline(memory=None,\n",
      "         steps=[('tfidfvectorizer',\n",
      "                 TfidfVectorizer(analyzer='word', binary=False,\n",
      "                                 decode_error='strict',\n",
      "                                 dtype=<class 'numpy.float64'>,\n",
      "                                 encoding='utf-8', input='content',\n",
      "                                 lowercase=True, max_df=1.0, max_features=500,\n",
      "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
      "                                 preprocessor=None, smooth_idf=True,\n",
      "                                 stop_words=None, strip_accents=None,\n",
      "                                 sublinear_tf=False,\n",
      "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                                 tokenizer=None, use_idf=True,\n",
      "                                 vocabulary=None)),\n",
      "                ('logisticregression',\n",
      "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
      "                                    fit_intercept=True, intercept_scaling=1,\n",
      "                                    l1_ratio=None, max_iter=100,\n",
      "                                    multi_class='auto', n_jobs=None,\n",
      "                                    penalty='l2', random_state=None,\n",
      "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                                    warm_start=False))],\n",
      "         verbose=False)\n",
      "\n",
      "Cross-val score of the best model: 0.9128840548160768\n",
      "Accuracy of best model on the training data: 0.9184250293837801\n",
      "Accuracy of best model on the testing data: 0.9133646742780389\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run gridsearch function on stemmed tokens\n",
    "\n",
    "gridsearch(TfidfVectorizer(), LogisticRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best parameters: stop_words = None, max_features = 500, ngrams = 1-grams<br>\n",
    "Cross-Val accuracy: 91.29%, Training accuracy: 91.84%, Testing accuracy: 91.33%\n",
    "Comparing the cross-val accuracy scores with the 91.28% of Model 2 (most accurate model up to this point) - Model 14 performed 0.01 percentage points better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the analysis above I hypertune the Logistic Regression model with Tfidf Vectorizer. Since lemmatizing and stemming produced similar results, I use only the basic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
